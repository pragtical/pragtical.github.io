"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[943],{47075:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>a,toc:()=>l});var i=t(85893),r=t(11151);const s={sidebar_position:41},o="core.tokenizer",a={id:"api/core.tokenizer",title:"core.tokenizer",description:"Functionality to tokenize source code using syntax definitions.",source:"@site/docs/api/core.tokenizer.md",sourceDirName:"api",slug:"/api/core.tokenizer",permalink:"/docs/api/core.tokenizer",draft:!1,unlisted:!1,editUrl:"https://github.com/pragtical/pragtical.github.io/edit/main/docs/api/core.tokenizer.md",tags:[],version:"current",sidebarPosition:41,frontMatter:{sidebar_position:41},sidebar:"tutorialSidebar",previous:{title:"core.titleview",permalink:"/docs/api/core.titleview"},next:{title:"core.view",permalink:"/docs/api/core.view"}},c={},l=[{value:"each_token",id:"each_token",level:2},{value:"extract_subsyntaxes",id:"extract_subsyntaxes",level:2},{value:"tokenize",id:"tokenize",level:2}];function d(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",hr:"hr",p:"p",pre:"pre",...(0,r.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"coretokenizer",children:"core.tokenizer"}),"\n",(0,i.jsx)(n.p,{children:"Functionality to tokenize source code using syntax definitions."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-lua",children:'local tokenizer = require "core.tokenizer"\n'})}),"\n",(0,i.jsx)(n.h2,{id:"each_token",children:"each_token"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-lua",children:"function core.tokenizer.each_token(t: any, scol: any)\n  -> fun(...any):...unknown\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"extract_subsyntaxes",children:"extract_subsyntaxes"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-lua",children:"function core.tokenizer.extract_subsyntaxes(base_syntax: table, state: string)\n  -> table\n"})}),"\n",(0,i.jsx)(n.p,{children:"Return the list of syntaxes used in the specified state."}),"\n",(0,i.jsxs)(n.p,{children:["@",(0,i.jsx)(n.em,{children:"param"})," ",(0,i.jsx)(n.code,{children:"base_syntax"})," \u2014 The initial base syntax (the syntax of the file)"]}),"\n",(0,i.jsxs)(n.p,{children:["@",(0,i.jsx)(n.em,{children:"param"})," ",(0,i.jsx)(n.code,{children:"state"})," \u2014 The state of the tokenizer to extract from"]}),"\n",(0,i.jsxs)(n.p,{children:["@",(0,i.jsx)(n.em,{children:"return"})," \u2014 Array of syntaxes starting from the innermost one"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"tokenize",children:"tokenize"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-lua",children:"function core.tokenizer.tokenize(incoming_syntax: table, text: string, state: string, resume: any)\n  -> table|unknown\n  2. string\n  3. table|nil\n"})}),"\n",(0,i.jsx)(n.hr,{})]})}function u(e={}){const{wrapper:n}={...(0,r.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>a,a:()=>o});var i=t(67294);const r={},s=i.createContext(r);function o(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);